#!/usr/bin/env python3
import argparse
import os
import subprocess
import sys
import uuid
import multiprocessing
import pathlib

DEFAULT_NUM_WORKERS = min(multiprocessing.cpu_count(), 30)


def parse_arguments():
    parser = argparse.ArgumentParser(description="Copy files in parallel.")
    parser.add_argument("source", help="source path")
    parser.add_argument("destination", help="destination path")
    parser.add_argument(
        "-j", default=DEFAULT_NUM_WORKERS, type=int, help="number of threads to use"
    )
    parser.add_argument(
        "-i",
        default=None,
        type=str,
        help="Same as ssh -i, specify which private key to use in ssh. Example: ~/.ssh/id_rsa. Default is None as not specified.",
    )
    parser.add_argument(
        "-c", default=1000, type=int, help="number of files in one chunk"
    )

    parser.add_argument(
        "--resume",
        default=False,
        help="resume previous job (skip finding files), pass in the ",
    )
    parser.add_argument(
        "--sudo", action="store_true", help="Add sudo to the rsync command"
    )
    parser.add_argument(
        "--dry", action="store_true", help="perform a trial run with no changes made"
    )
    parser.add_argument("--lfs", action="store_true", help="use lustre utilities")
    parser.add_argument(
        "--arcfour", action="store_true", help="use arcfour as compression algorithm"
    )
    parser.add_argument("--tmp", default="/tmp/", help="path to store temporary data")
    parser.add_argument(
        "--filter", default="*", help="The file matcher to filter which files to copy"
    )
    parser.add_argument(
        "--test-ssh",
        "-ts",
        action="store_true",
        help="Test the SSH connection to the source",
    )

    args = parser.parse_args()
    return args


def find_files_create_job(args, source, destination, files_file, tmp_dir, jobs_file):
    # prepare find exe
    if args.lfs:
        find = "lfs find"
    else:
        find = "find"

    # find files
    find_cmd = f'{find} . -name \\"{args.filter}\\" -type f'
    if ":" in source:
        # Hacky, but it works. If source is a remote machine (so it's
        # given with user@machine:/path/to/dir) we just split it at the ":"
        # This way we can use both parts of the string with the remote find
        # command.
        source_um, source_path = source.split(":")
        subprocess.call(
            f'ssh {source_um} "cd {source_path}; {find_cmd}" > {files_file}', shell=True
        )
    else:
        # Source is a local directory.
        subprocess.call(f"{find_cmd} > {files_file}", shell=True, cwd=source)

    # split the files file in chunks of 1000
    chunk_file = os.path.join(tmp_dir, "chunk")
    subprocess.call(f"split -l {args.c} -d -a 8 {files_file} {chunk_file}", shell=True)

    # create a file containing all the copy jobs to perform
    with open(jobs_file, "w") as jobs_file_handler:
        chunk_files = sorted([f for f in os.listdir(tmp_dir) if f.startswith("chunk")])
        for filename in chunk_files:
            chunk_file = os.path.join(tmp_dir, filename)
            chunk_num = filename.split("chunk")[1]
            log_file = os.path.join(tmp_dir, "log" + chunk_num)

            job = "rsync -aH --no-l"
            if args.sudo:
                job = "sudo " + job
            # SSH options
            if args.i is not None or args.arcfour:
                ssh_opt = ""
                if args.i is not None:
                    ssh_opt += f" -i {args.i} "
                if args.arcfour:
                    ssh_opt += " -c arcfour "
                job += f" -e 'ssh {ssh_opt}'"

            job += f" --files-from={chunk_file} --log-file={log_file} {source} {destination}\n"

            jobs_file_handler.write(job)


def test_connection(args):
    source = args.source
    ssh_target = source.split(":")[0]
    subprocess.call(f"ssh {ssh_target} 'echo SSH connected to `hostname`'", shell=True)


def main():
    args = parse_arguments()

    if args.test_ssh:
        test_connection(args)
        return

    # check if GNU parallel is installed
    if not any(
        os.access(os.path.join(path, "parallel"), os.X_OK)
        for path in os.environ["PATH"].split(os.pathsep)
    ):
        raise Exception("GNU parallel is not installed.")

    # remove trailing slashes
    source = args.source.rstrip("/")
    destination = args.destination.rstrip("/")

    if args.resume:
        # prepare files/directories
        tmp_dir = pathlib.Path(args.resume)
        if not tmp_dir.is_dir():
            raise RuntimeError(
                f"Error!!: Can not find jobs tmp directory to resume, {tmp_dir.absolute()}"
            )
    else:
        # prepare files/directories
        tmp_dir = (
            pathlib.Path(args.tmp) / "copy-files-in-parallel-cache" / str(uuid.uuid4())
        )
        tmp_dir.mkdir(parents=True)

    files_file = tmp_dir / "files"
    jobs_file = tmp_dir / "jobs"

    if not args.resume:
        find_files_create_job(args, source, destination, files_file, tmp_dir, jobs_file)

    n_files = int(subprocess.check_output(f"wc -l {files_file}", shell=True).split()[0])
    n_jobs = int(subprocess.check_output(f"wc -l {jobs_file}", shell=True).split()[0])

    print(f"Copying {n_files} files using {n_jobs} jobs!\n")
    print(f"To resume this session, add: --resume {tmp_dir}\n\n\n")

    parallel_log = tmp_dir / "parallel.log"
    print(f"Parallel log file: {parallel_log.absolute()}\n")

    # run parallel with the jobs file
    subprocess.call(
        f"cat {jobs_file} | parallel --will-cite -P{args.j} --bar --progress --joblog {parallel_log.absolute()}",
        shell=True,
    )


if __name__ == "__main__":
    main()
