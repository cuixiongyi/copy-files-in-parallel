#!/usr/bin/env python
import argparse
import os
import subprocess
import sys
import uuid
import multiprocessing
import pathlib

DEFAULT_NUM_WORKERS = int(multiprocessing.cpu_count()*1.3)


parser = argparse.ArgumentParser(description="Copy files in parallel.")
parser.add_argument("source", help="source path")
parser.add_argument("destination", help="destination path")
parser.add_argument("-j", default=DEFAULT_NUM_WORKERS, type=int, help="number of threads to use")
parser.add_argument(
    "-i",
    default=None,
    type=str,
    help="Same as ssh -i, specify which private key to use in ssh. Example: ~/.ssh/id_rsa. Default is None as not specified.",
)
parser.add_argument("-c", default=1000, type=int, help="number of files in one chunk")

parser.add_argument("--resume", default=False, help="resume previous job (skip finding files), pass in the ")
parser.add_argument("--sudo", action="store_true", help="Add sudo to the rsync command")
parser.add_argument("--dry", action="store_true", help="perform a trial run with no changes made")
parser.add_argument("--lfs", action="store_true", help="use lustre utilities")
parser.add_argument("--arcfour", action="store_true", help="use arcfour as compression algorithm")
parser.add_argument("--tmp", default="/tmp/", help="path to store temporary data")
parser.add_argument("--filter", default="*", help="The file matcher to filter which files to copy")

args = parser.parse_args()

# check if GNU parallel is installed
if not any(os.access(os.path.join(path, "parallel"), os.X_OK) for path in os.environ["PATH"].split(os.pathsep)):
    raise Exception("GNU parallel is not installed.")

# remove trailing slashes
source = args.source.rstrip("/")
destination = args.destination.rstrip("/")

# prepare find exe
if args.lfs:
    find = "lfs find"
else:
    find = "find"


if args.resume:
    # prepare files/directories
    tmp_dir = os.path.join(args.resume)
    if not os.path.isdir(tmp_dir):
        sys.exit("Error: Can not find jobs tmp directory.")

    files_file = os.path.join(tmp_dir, "files")
    jobs_file = os.path.join(tmp_dir, "jobs")

else:
    # prepare files/directories
    tmp_dir = pathlib.Path(args.tmp)/ "copy-files-in-parallel-cache"/ str(uuid.uuid4())
    tmp_dir.mkdir(parents=True)

    files_file = os.path.join(tmp_dir, "files")
    jobs_file = os.path.join(tmp_dir, "jobs")

    # find files
    find_cmd = f'{find} . -name \\"{args.filter}\\" -type f'
    if ":" in source:
        # Hacky, but it works. If source is a remote machine (so it's
        # given with user@machine:/path/to/dir) we just split it at the ":"
        # This way we can use both parts of the string with the remote find
        # command.
        source_um, source_path = source.split(":")
        subprocess.call(f'ssh {source_um} "cd {source_path}; {find_cmd}" > {files_file}', shell=True)
    else:
        # Source is a local directory.
        subprocess.call(f'{find_cmd} > {files_file}', shell=True, cwd=source)

    # split the files file in chunks of 1000
    chunk_file = os.path.join(tmp_dir, "chunk")
    subprocess.call(f"split -l {args.c} -d -a 8 {files_file} {chunk_file}", shell=True)

    # create a file containing all the copy jobs to perform
    jobs_file_handler = open(jobs_file, "w")
    chunk_files=sorted(os.listdir(tmp_dir))
    for filename in chunk_files:
        if filename.startswith("chunk"):
            chunk_file = os.path.join(tmp_dir, filename)
            chunk_num = filename.split("chunk")[1]
            log_file = os.path.join(tmp_dir, "log" + chunk_num)

            job = "rsync -aH --no-l"
            if args.sudo:
                job = "sudo " + job
            # SSH options
            if args.i is not None or args.arcfour:
                ssh_opt = ""
                if args.i is not None:
                    ssh_opt += f" -i {args.i} "
                if args.arcfour:
                    ssh_opt += " -c arcfour "
                job += f" -e 'ssh {ssh_opt}'"

            job += f" --files-from={chunk_file} --log-file={log_file} {source} {destination}\n"

            jobs_file_handler.write(job)

    jobs_file_handler.close()

n_files = int(subprocess.check_output(f"wc -l {files_file}", shell=True).split()[0])
n_jobs = int(subprocess.check_output(f"wc -l {jobs_file}" , shell=True).split()[0])

print(f"Copying {n_files} files using {n_jobs} jobs! {len(chunk_files)} chunks\n")
print(f'To resume this session, add: --resume {tmp_dir}\n\n\n')

parallel_log=tmp_dir/'parallel.log'
print(f'Parallel log file: {parallel_log.absolute()}\n')

# run parallel with the jobs file
subprocess.call(f"cat {jobs_file} | parallel -P{args.j} --bar --progress --joblog {parallel_log.absolute()}" , shell=True)
